#!/usr/bin/env python2
"""Main utilites
divers tools to be used across modules

Notes:
    forked from https://git.sonova.com/11dbiner/python-pack-util.git
"""

import os
import sys
import fnmatch
import json
import logging
import stat
import subprocess
import traceback

from collections import OrderedDict
from hashlib import md5, sha1, sha256
from xml.dom import minidom
from xml.etree import ElementTree as ET

from .xmljson import BadgerFish, GData, Yahoo, Parker

_PIPE = subprocess.PIPE
_NIN = ' --non-interactive'
_PACK_DIR = os.path.abspath(os.path.dirname(__file__))
_DOC_URL = r'https://pages.git.sonova.com/embedded-software/python-dsp-tools'


###############################################################################
def get_config(name, as_dict=False):
    """Summary

    Returns:
        TYPE: Description
    """
    files = {
        'logging': 'logging.json',
    }
    filename = os.path.join(_PACK_DIR, 'config', files[name])
    if as_dict:
        with open(filename, 'r') as conf_:
            config = json.load(conf_)
        return config
    return filename


def docs():
    """Open documentation (GitHub Pages) with the default browser
    """
    plat_ = sys.platform
    if plat_ == "linux" or plat_ == "linux2" or plat_ == "darwin":
        # linux or MAC OS X
        os.system('open %s' % _DOC_URL)
    elif plat_ == "win32":
        os.system('start "" %s' % _DOC_URL)


###############################################################################
def force_rmtree(root_dir):
    '''
    rmtree doesn't work when no write bit in linux or read-only in windows
    force_rmtree recursively walk, do chmod and then remove
    src: https://gist.github.com/jagt/6759127

    Args:
        root_dir (TYPE): Description
    '''
    for root, dirs, files in os.walk(root_dir, topdown=False):
        for name in files:
            file_path = os.path.join(root, name)
            os.chmod(file_path, stat.S_IWUSR)
            os.remove(file_path)
        for name in dirs:
            dir_path = os.path.join(root, name)
            os.chmod(dir_path, stat.S_IWUSR)
            os.rmdir(dir_path)
    os.rmdir(root_dir)


###############################################################################
def parse_xml(data, parser=BadgerFish):
    """parse xml data (str) to json data (dict)

    Args:
        data (TYPE): Description
        parser (TYPE, optional): Description

    Returns:
        TYPE: Description
    """
    parser = parser(dict_type=OrderedDict)
    return parser.data(ET.fromstring(data))


def unparse_xml(data, root=None, pretty=True, encoding='utf-8', parser=BadgerFish):
    """unparse json data (dict) to xml data (str)

    Args:
        data (dict, OrderedDict): dict of data, first level should has one key that is used as key, if not give a root
        root (dict, ElementTree.Element): root to be used to create the xml data, dict structure: {'name': <name>, 'attrib': dict()}), attrib optional
        pretty (bool, optional): pretty formatting
        encoding (str, optional): encoding used, default 'utf-8'
        parser (TYPE, optional): specific parser from xmljson package

    Returns:
        str: encoded xml string, can be written directly to a file

    Raises:
        ValueError: if no root is given and len(data)!=1, inconsistent data
    """
    parser = parser(dict_type=OrderedDict)
    if root is None:
        if len(data) != 1:
            raise ValueError('the data has too many roots, adapt the data or give an ElementTree Element as root')
        root = ET.Element(data.keys()[0])
        data = data[data.keys()[0]]
    elif isinstance(root, dict):
        if 'attrib' in root:
            root = ET.Element(root['name'], attrib=root['attrib'])
        else:
            root = ET.Element(root['name'])
    xml_str = ET.tostring(parser.etree(data, root=root))
    if pretty:
        xml_str = minidom.parseString(xml_str).toprettyxml(indent=' '*4)
    xml_str = xml_str.replace(r'<?xml version="1.0" ?>',
                              r'<?xml version="1.0" encoding="' + encoding + r'"?>', 1)
    return xml_str.encode(encoding)


###############################################################################
def print_exc():
    """print exception using traceback
    if logger is used, use logger.exception('...')
    """
    print '-'*60
    traceback.print_exc(file=sys.stdout)
    print '-'*60


###############################################################################
def unique(seq, idfun=None):
    """unique with order preserving

    Args:
        seq (list): list to be cleaned
        idfun (function, optional): identification function

    Returns:
        TYPE: Description
    """
    if len(seq) == 0:
        return seq
    if idfun is None:
        def idfun(val):
            return val
    seen = {}
    result = []
    for item in seq:
        marker = idfun(item)
        if marker in seen:
            continue
        seen[marker] = 1
        result.append(item)
    result.sort()
    return result


###############################################################################
def hashit(string, algo='md5'):
    """hash string or file with a specific algorithm

    Args:
        string (str): string to be hashed, use file::/path/to/file to hash a files content
        algo (str, optional): hash type, default md5, supported md5, sha1, sha256

    Returns:
        TYPE: Description
    """
    logger = logging.getLogger(__name__)
    logger.debug('hashing "%s" with algorithm "%s"', string, algo)
    if not isinstance(string, str) and not isinstance(string, unicode):
        logger.error('first argument is not a string, aborting.')
        return
    # check if file or string
    filemode = bool(string[0:6] == 'file::')
    # get algo object
    if algo.lower() == 'MD5'.lower():
        algo = md5()
    elif algo.lower() == 'SHA1'.lower():
        algo = sha1()
    elif algo.lower() == 'SHA256'.lower():
        algo = sha256()
    else:
        logger.error('hash algorithm "%s" unknown, aborting.', algo)
    # hash it
    if filemode:
        with open(string[6:], 'rb') as file_:
            for chunk in iter(lambda: file_.read(128 * algo.block_size), b''):
                algo.update(chunk)
    else:
        algo.update(string)
    return algo.hexdigest()


def hashdeep(dirs=None, rec=True, files=None, output=None, relpath=''):
    """create hashes of a list of files or from all files within a given directory
    output is in hashdeep format

    Args:
        dirs (list,str, optional): Description
        rec (bool, optional): Description
        files (None, optional): Description
        output (None, optional): Description
        relpath (str, optional): Description

    Returns:
        TYPE: Description
    """
    logger = logging.getLogger(__name__)
    # prepare output
    header = [r'%%%% HASHDEEP-1.0',
              r'%%%% size,md5,sha256,filename',
              r'##',
              r'## util.hashdeep',
              r'## ']
    if output:
        out = open(output, 'w')
        out.write('\n'.join(header) + '\n')
    else:
        lines_ = list()
        lines_.append(header)
    # prepare files structure
    files_ = list()
    if isinstance(files, str):
        files_.append(files)
    elif files:
        files_ = files_ + files
    # append files within dirs
    if isinstance(dirs, str):
        dirs = [dirs]
    for dir_ in dirs:
        if dir_:
            rec_list(dir_, files=files_, rec=rec)
    # go through files
    for file_ in files_:
        size = os.path.getsize(file_)
        md5 = hashit('file::' + file_, algo='md5')
        sh2 = hashit('file::' + file_, algo='sha256')
        name = os.path.relpath(file_, relpath)
        line = '%s,%s,%s,%s' % (size, md5, sh2, name)
        if output:
            out.write(line + '\n')
        else:
            lines_.append(line)
    if output:
        out.close()
    else:
        logger.info('hashdeep output:\n%s', '\n'.join(lines_))
    return


def hashdeep_audit(path, filename=None):
    """audit file corruption with hashdeep

    Args:
        path (str): export path
        filename (None, optional): Description

    Returns:
        bool: True if successful
    """
    logger = logging.getLogger(__name__)
    if not filename:
        filename = 'hashdeep.csv'
    plat_ = sys.platform
    if plat_ == "linux" or plat_ == "linux2" or plat_ == "darwin":
        # linux or MAC OS X
        cmd = 'cd %s && hashdeep -ravlk %s */*' % (path, filename)
    elif plat_ == "win32":
        # our beloved Windows
        dirs = list()
        rec_list(path, dirs=dirs, rec=False)
        dirs_str = ''
        for dir_ in dirs:
            dirs_str = dirs_str + '"' + os.path.relpath(dir_, path) + '\\*" '
        dirs_str = dirs_str[:-1]
        cmd = 'cd %s & hashdeep64 -ravlk %s %s' % (path, filename, dirs_str)
    else:
        logger.error('unknown platform "%s", aborting', plat_)
        return False
    return run_cmd(cmd, 'HASHDEEP audit')


###############################################################################
def svn_info(path, xml=False):
    """Summary

    Args:
        path (TYPE): Description
        xml (bool, optional): Description

    Returns:
        TYPE: Description
    """
    if xml:
        cmd = 'svn info --xml "%s"' % path
    else:
        cmd = 'svn info "%s"' % path
    return run_cmd(cmd + _NIN, 'SVN info')


def svn_checkout(url, path, args=None):
    """svn checkout

    Args:
        url (TYPE): Description
        path (str): path to svn (sub)repository

    Returns:
        bool: True if successful
    """
    if args:
        cmd = 'svn checkout "%s" "%s" %s' % (url, path, args)
    else:
        cmd = 'svn checkout "%s" "%s"' % (url, path)
    return run_cmd(cmd + _NIN, 'SVN checkout')


def svn_cleanup(path):
    """svn cleanup

    Args:
        path (str): path to svn (sub)repository

    Returns:
        bool: True if successful
    """
    cmd = 'svn cleanup "%s"' % path
    cmd = cmd + ' --remove-unversioned --remove-ignored'
    return run_cmd(cmd + _NIN, 'SVN cleanup')


def svn_revert(path):
    """svn revert

    Args:
        path (str): path to svn (sub)repository

    Returns:
        bool: True if successful
    """
    cmd = 'svn revert -R "%s"' % path
    return run_cmd(cmd + _NIN, 'SVN revert')


def svn_update(path, revision=None):
    """svn update path

    Args:
        path (str): path to svn (sub)repository
        revision (str or int, optional): update to specific revision

    Returns:
        bool: True if successful
    """
    if not revision:
        revision = 'HEAD'
    if isinstance(revision, str):
        cmd = 'svn update "%s" -r %s --depth infinity' % (path, revision)
    else:
        cmd = 'svn update "%s" -r %i --depth infinity' % (path, revision)
    return run_cmd(cmd + _NIN, 'SVN update')


def svn_commit(path, message):
    """svn commit path

    Args:
        path (str): path to svn (sub)repository
        message (TYPE): Description

    Returns:
        bool: True if successful

    Deleted Parameters:
        revision (str or int, optional): update to specific revision
    """
    cmd = 'svn commit "%s" -m "%s"' % (path, message)
    return run_cmd(cmd + _NIN, 'SVN commit')


def svn_lock(path, message):
    """svn lock path

    Args:
        path (str): path to svn (sub)repository
        message (TYPE): Description

    Returns:
        bool: True if successful

    Deleted Parameters:
        revision (str or int, optional): update to specific revision
    """
    cmd = 'svn lock "%s" -m "%s"' % (path, message)
    return run_cmd(cmd + _NIN, 'SVN commit')


def svn_unlock(path):
    """svn lock path

    Args:
        path (str): path to svn (sub)repository

    Returns:
        bool: True if successful

    Deleted Parameters:
        revision (str or int, optional): update to specific revision
    """
    cmd = 'svn unlock --force "%s"' % path
    return run_cmd(cmd + _NIN, 'SVN unlock')


def run_cmd(cmd, title=None):
    """Summary

    Args:
        cmd (TYPE): Description
        title (None, optional): Description

    Returns:
        TYPE: Description
    """
    logger = logging.getLogger(__name__)
    logger.debug('cmd.exe "%s"', cmd)
    if not title:
        title = cmd
    prc = subprocess.Popen(cmd, stdout=_PIPE, stderr=_PIPE,
                           stdin=_PIPE, shell=True)
    std = prc.communicate()
    status = prc.returncode
    if status != 0:
        logger.debug('command "%s" unsuccessful with output:\n%s', title, std[0])
        logger.error('error during command "%s":\n%s', title, std[1])
    return (status, std)


###############################################################################
def nprint(data, return_data=False):
    """nice print for dict/list/json

    Args:
        data (TYPE): Data to be dumped
        return_data (bool, optional): Description

    Returns:
        TYPE: Description
    """
    logger = logging.getLogger(__name__)
    try:
        data = json.dumps(data, indent=4, cls=MyEncoder,
                          sort_keys=False, separators=(',', ':'))
    except:
        logger.exception('error during json dump.')
    if return_data:
        return data
    else:
        print data


class MyEncoder(json.JSONEncoder):

    """encoder to handle special instances
    """

    def default(self, obj):
        """default conversion strategy

        Args:
            obj (TYPE): Description

        Returns:
            TYPE: Description
        """
        logger = logging.getLogger(__name__ + '.' + self.__class__.__name__)
        try:
            return str(obj)
        except:
            logger.exception('error with the custom encoder.')
        return json.JSONEncoder.default(self, obj)


###############################################################################
def rec_list(rootpath, expr='*', files=None, dirs=None, rec=True):
    """Recursive directory scan
    expression is not regexp

    Args:
        rootpath (str): root path
        expr (str, optional): search expression (not regexp)
        files (list, optional): list instance to save all found files
        dirs (list, optional): list instance to save all found directories
        rec (bool, optional): enable recursive mode, default "True"
    """
    logger = logging.getLogger(__name__)
    try:
        if rec:
            for root, dirnames, filenames in os.walk(rootpath):
                if files is not None:
                    for filename in fnmatch.filter(filenames, expr):
                        files.append(os.path.join(root, filename))
                if dirs is not None:
                    for dirname in fnmatch.filter(dirnames, expr):
                        dirs.append(os.path.join(root, dirname))
        else:
            root, dirnames, filenames = next(os.walk(rootpath))
            if files is not None:
                for filename in fnmatch.filter(filenames, expr):
                    files.append(os.path.join(root, filename))
            if dirs is not None:
                for dirname in fnmatch.filter(dirnames, expr):
                    dirs.append(os.path.join(root, dirname))
    except:
        logger.exception('an error occurred during the walk in %s', rootpath)


###############################################################################
class Tee(object):

    """class to print in file and on console

    Attributes:
        files (TYPE): Description
    """

    def __init__(self, *files):
        """Summary

        Args:
            *files (TYPE): Output file(s)
        """
        self.files = files

    def write(self, obj):
        """Summary

        Args:
            obj (TYPE): Description
        """
        for file_ in self.files:
            file_.write(obj)

    def read(self, obj):
        """Summary

        Args:
            obj (TYPE): Description
        """
        for file_ in self.files:
            file_.read(obj)


###############################################################################
if __name__ == '__main__':
    sys.exit(0)
